"""
Tests for agentic_kg.agents.evaluation.

Generated by test-generator agent.
"""

from types import SimpleNamespace
from unittest.mock import MagicMock

import pytest

from agentic_kg.agents.evaluation import EvaluationAgent
from agentic_kg.agents.sandbox import SandboxResult
from agentic_kg.agents.schemas import ContinuationProposal, EvaluationResult


class TestEvaluationAgent:
    """Tests for EvaluationAgent."""

    @pytest.fixture
    def mock_sandbox(self):
        sandbox = MagicMock()
        sandbox.execute.return_value = SandboxResult(
            success=True,
            stdout='{"metrics": {"accuracy": 0.91}}',
            stderr="",
            exit_code=0,
        )
        return sandbox

    @pytest.fixture
    def agent(self, mock_llm, mock_repo, mock_search, mock_relations, mock_sandbox):
        return EvaluationAgent(
            llm_client=mock_llm,
            repository=mock_repo,
            search_service=mock_search,
            relation_service=mock_relations,
            sandbox=mock_sandbox,
        )

    def test_name(self, agent):
        assert agent.name == "evaluation"

    @pytest.mark.asyncio
    async def test_run_happy_path(self, agent, mock_llm, state_with_proposal):
        """Run generates code, executes, and interprets results."""
        mock_llm.extract.return_value = SimpleNamespace(content="print('hello')")

        result = await agent.run(state_with_proposal)

        assert result["evaluation_result"] is not None
        assert result["evaluation_result"]["execution_success"] is True
        assert result["evaluation_result"]["verdict"] == "inconclusive"
        assert result["evaluation_approved"] is False

    @pytest.mark.asyncio
    async def test_run_no_proposal(self, agent, initial_state):
        """Run returns error when no proposal exists."""
        result = await agent.run(initial_state)
        assert "No proposal" in result["errors"][-1]

    @pytest.mark.asyncio
    async def test_run_sandbox_failure(self, agent, mock_llm, mock_sandbox, state_with_proposal):
        """Handles sandbox execution failure."""
        mock_sandbox.execute.return_value = SandboxResult(
            success=False,
            stdout="",
            stderr="ImportError: no module named foo",
            exit_code=1,
        )
        mock_llm.extract.return_value = SimpleNamespace(content="import foo")

        result = await agent.run(state_with_proposal)

        assert result["evaluation_result"]["execution_success"] is False
        assert result["evaluation_result"]["verdict"] == "inconclusive"

    @pytest.mark.asyncio
    async def test_run_sandbox_timeout(self, agent, mock_llm, mock_sandbox, state_with_proposal):
        """Handles sandbox timeout."""
        mock_sandbox.execute.return_value = SandboxResult(
            success=False,
            stdout="",
            stderr="timed out",
            exit_code=-1,
            timed_out=True,
        )
        mock_llm.extract.return_value = SimpleNamespace(content="while True: pass")

        result = await agent.run(state_with_proposal)

        assert result["evaluation_result"]["verdict"] == "not_viable"
        assert result["evaluation_result"]["feasibility_score"] == 0.1

    @pytest.mark.asyncio
    async def test_run_strips_markdown_code_fences(self, agent, mock_llm, mock_sandbox, state_with_proposal):
        """Code extraction strips markdown fences."""
        mock_llm.extract.return_value = SimpleNamespace(
            content="```python\nprint('hello')\n```"
        )

        await agent.run(state_with_proposal)

        # The code passed to sandbox should not have fences
        call_args = mock_sandbox.execute.call_args[0][0]
        assert "```" not in call_args
        assert "print('hello')" in call_args

    @pytest.mark.asyncio
    async def test_run_handles_llm_error(self, agent, mock_llm, state_with_proposal):
        """Handles LLM error gracefully."""
        mock_llm.extract.side_effect = RuntimeError("LLM error")

        result = await agent.run(state_with_proposal)

        assert any("LLM error" in e for e in result["errors"])

    def test_sandbox_lazy_init(self, mock_llm, mock_repo):
        """Sandbox is lazily initialized if not provided."""
        agent = EvaluationAgent(
            llm_client=mock_llm,
            repository=mock_repo,
        )
        assert agent._sandbox is None


class TestInterpretResults:
    """Tests for _interpret_results logic via the agent run path."""

    @pytest.fixture
    def mock_sandbox(self):
        sandbox = MagicMock()
        return sandbox

    @pytest.fixture
    def agent(self, mock_llm, mock_repo, mock_sandbox):
        return EvaluationAgent(
            llm_client=mock_llm,
            repository=mock_repo,
            sandbox=mock_sandbox,
        )

    @pytest.mark.asyncio
    async def test_metrics_with_improvement(self, agent, mock_llm, mock_sandbox, mock_problem):
        """Metrics with improvement over baseline yield 'promising'."""
        # Add a metric baseline to the problem
        metric = SimpleNamespace(name="accuracy", baseline_value=0.85, description="")
        mock_problem.metrics = [metric]
        mock_problem.datasets = []

        mock_sandbox.execute.return_value = SandboxResult(
            success=True,
            stdout='{"metrics": {"accuracy": 0.91}}',
            stderr="",
            exit_code=0,
        )
        mock_llm.extract.return_value = SimpleNamespace(content="print('test')")

        from agentic_kg.agents.state import create_initial_state
        state = create_initial_state()
        state = {
            **state,
            "selected_problem_id": "prob-1",
            "proposal": {
                "problem_id": "prob-1",
                "title": "Test Proposal Title",
                "methodology": "A methodology that is long enough here.",
                "expected_outcome": "Better results expected",
                "experimental_steps": [],
                "required_resources": [],
                "metrics_to_evaluate": [],
                "confidence": 0.5,
                "rationale": "",
            },
            "proposal_approved": True,
        }

        result = await agent.run(state)

        eval_result = result["evaluation_result"]
        assert eval_result["verdict"] == "promising"
        assert eval_result["feasibility_score"] == 0.8
        assert len(eval_result["metrics_results"]) == 1
        assert eval_result["metrics_results"][0]["improvement"] is not None
