"""
Shared fixtures for agent tests.

Generated by test-generator agent.
"""

import uuid
from dataclasses import dataclass, field
from types import SimpleNamespace
from typing import Any, Optional
from unittest.mock import AsyncMock, MagicMock

import pytest

from agentic_kg.agents.schemas import (
    ContinuationProposal,
    EvaluationResult,
    ExperimentalStep,
    MetricResult,
    RankedProblem,
    RankingResult,
    SynthesisReport,
    WorkflowStatus,
)
from agentic_kg.agents.state import ResearchState, create_initial_state


# =============================================================================
# Mock LLM Client
# =============================================================================


@pytest.fixture
def mock_llm():
    """Mock LLM client with extract() and structured_extract() async methods."""
    llm = MagicMock()
    llm.extract = AsyncMock()
    llm.structured_extract = AsyncMock()
    return llm


# =============================================================================
# Mock Repository
# =============================================================================


def _make_problem(
    id: str = "prob-1",
    statement: str = "How to improve GNN scalability?",
    domain: str = "graph_ml",
    status_value: str = "open",
):
    """Create a mock problem object."""
    problem = MagicMock()
    problem.id = id
    problem.statement = statement
    problem.domain = domain
    problem.status = SimpleNamespace(value=status_value)
    problem.constraints = []
    problem.datasets = []
    problem.baselines = []
    problem.metrics = []
    problem.extraction_metadata = None
    return problem


@pytest.fixture
def mock_problem():
    """A single mock problem."""
    return _make_problem()


@pytest.fixture
def mock_repo(mock_problem):
    """Mock Neo4jRepository."""
    repo = MagicMock()
    repo.get_problem.return_value = mock_problem
    repo.list_problems.return_value = [mock_problem]
    repo.create_problem.return_value = mock_problem
    repo.update_problem.return_value = mock_problem
    return repo


# =============================================================================
# Mock Search and Relation Services
# =============================================================================


@pytest.fixture
def mock_search(mock_problem):
    """Mock SearchService."""
    search = MagicMock()
    result = SimpleNamespace(problem=mock_problem)
    search.structured_search.return_value = [result]
    return search


@pytest.fixture
def mock_relations():
    """Mock RelationService."""
    relations = MagicMock()
    relations.get_related_problems.return_value = [
        {"type": "EXTENDS", "statement": "Related problem statement"},
    ]
    relations.create_relation.return_value = None
    return relations


# =============================================================================
# State Fixtures
# =============================================================================


@pytest.fixture
def initial_state() -> ResearchState:
    """Fresh initial state."""
    return create_initial_state(domain_filter="graph_ml")


@pytest.fixture
def state_with_ranked_problems(initial_state) -> ResearchState:
    """State after ranking step with ranked problems."""
    return {
        **initial_state,
        "ranked_problems": [
            RankedProblem(
                problem_id="prob-1",
                statement="How to improve GNN scalability?",
                score=0.85,
                tractability=0.8,
                data_availability=0.9,
                cross_domain_impact=0.7,
                rationale="High tractability with good data availability.",
                domain="graph_ml",
            ).model_dump(),
        ],
        "total_candidates": 5,
        "current_step": "ranking",
    }


@pytest.fixture
def state_with_selected_problem(state_with_ranked_problems) -> ResearchState:
    """State after problem selection."""
    return {
        **state_with_ranked_problems,
        "selected_problem_id": "prob-1",
        "selected_problem_statement": "How to improve GNN scalability?",
    }


@pytest.fixture
def sample_proposal() -> ContinuationProposal:
    """A sample continuation proposal."""
    return ContinuationProposal(
        problem_id="prob-1",
        title="Scalable GNN via Subgraph Sampling",
        methodology="Apply neighbor sampling with importance weighting to reduce computation.",
        expected_outcome="2x speedup with <5% accuracy loss",
        experimental_steps=[
            ExperimentalStep(
                step_number=1,
                description="Implement importance-weighted neighbor sampling",
                expected_output="Sampling module code",
            ),
            ExperimentalStep(
                step_number=2,
                description="Benchmark on OGB datasets",
                expected_output="Accuracy and runtime metrics",
            ),
        ],
        confidence=0.7,
    )


@pytest.fixture
def state_with_proposal(state_with_selected_problem, sample_proposal) -> ResearchState:
    """State after continuation step."""
    return {
        **state_with_selected_problem,
        "proposal": sample_proposal.model_dump(mode="json"),
        "proposal_approved": True,
    }


@pytest.fixture
def sample_eval_result() -> EvaluationResult:
    """A sample evaluation result."""
    return EvaluationResult(
        proposal_id="prob-1",
        feasibility_score=0.8,
        code_generated="",
        execution_output="Accuracy: 0.91",
        execution_success=True,
        metrics_results=[
            MetricResult(name="accuracy", value=0.91, baseline_value=0.85, improvement=0.07),
        ],
        verdict="promising",
    )


@pytest.fixture
def state_with_evaluation(state_with_proposal, sample_eval_result) -> ResearchState:
    """State after evaluation step."""
    return {
        **state_with_proposal,
        "evaluation_result": sample_eval_result.model_dump(mode="json"),
        "evaluation_approved": True,
    }
