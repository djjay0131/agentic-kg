"""
Tests for agentic_kg.agents.schemas.

Generated by test-generator agent.
"""

import pytest
from datetime import datetime, timezone
from pydantic import ValidationError

from agentic_kg.agents.schemas import (
    CheckpointDecision,
    CheckpointType,
    ContinuationProposal,
    EvaluationResult,
    ExperimentalStep,
    GraphUpdate,
    HumanCheckpoint,
    MetricResult,
    RankedProblem,
    RankingResult,
    SynthesisReport,
    WorkflowStatus,
    WorkflowSummary,
)


# =============================================================================
# Enums
# =============================================================================


class TestEnums:
    """Tests for enum types."""

    def test_checkpoint_type_values(self):
        """CheckpointType has expected values."""
        assert CheckpointType.SELECT_PROBLEM == "select_problem"
        assert CheckpointType.APPROVE_PROPOSAL == "approve_proposal"
        assert CheckpointType.REVIEW_EVALUATION == "review_evaluation"

    def test_checkpoint_decision_values(self):
        """CheckpointDecision has expected values."""
        assert CheckpointDecision.APPROVE == "approve"
        assert CheckpointDecision.REJECT == "reject"
        assert CheckpointDecision.EDIT == "edit"

    def test_workflow_status_values(self):
        """WorkflowStatus has expected values."""
        assert WorkflowStatus.PENDING == "pending"
        assert WorkflowStatus.COMPLETED == "completed"
        assert WorkflowStatus.FAILED == "failed"


# =============================================================================
# RankedProblem
# =============================================================================


class TestRankedProblem:
    """Tests for RankedProblem model."""

    def test_create_valid(self):
        """Create with valid data."""
        rp = RankedProblem(
            problem_id="p1",
            statement="Test problem",
            score=0.8,
            tractability=0.7,
            data_availability=0.9,
            cross_domain_impact=0.6,
            rationale="This is a good rationale for ranking.",
        )
        assert rp.problem_id == "p1"
        assert rp.score == 0.8
        assert rp.domain is None
        assert rp.related_problem_count == 0

    @pytest.mark.parametrize("field_name,bad_value", [
        ("score", -0.1),
        ("score", 1.1),
        ("tractability", -0.5),
        ("tractability", 2.0),
        ("data_availability", -1),
        ("cross_domain_impact", 1.5),
    ])
    def test_rejects_out_of_range_scores(self, field_name, bad_value):
        """Score fields must be in [0, 1]."""
        data = {
            "problem_id": "p1",
            "statement": "Test",
            "score": 0.5,
            "tractability": 0.5,
            "data_availability": 0.5,
            "cross_domain_impact": 0.5,
            "rationale": "Valid rationale here.",
        }
        data[field_name] = bad_value
        with pytest.raises(ValidationError):
            RankedProblem(**data)

    def test_rationale_min_length(self):
        """Rationale must be at least 10 chars."""
        with pytest.raises(ValidationError):
            RankedProblem(
                problem_id="p1",
                statement="Test",
                score=0.5,
                tractability=0.5,
                data_availability=0.5,
                cross_domain_impact=0.5,
                rationale="short",
            )

    def test_boundary_scores_accepted(self):
        """Scores of exactly 0.0 and 1.0 are valid."""
        rp = RankedProblem(
            problem_id="p1",
            statement="Test",
            score=0.0,
            tractability=1.0,
            data_availability=0.0,
            cross_domain_impact=1.0,
            rationale="Boundary test rationale.",
        )
        assert rp.score == 0.0
        assert rp.tractability == 1.0


# =============================================================================
# RankingResult
# =============================================================================


class TestRankingResult:
    """Tests for RankingResult model."""

    def test_empty_defaults(self):
        """RankingResult works with all defaults."""
        rr = RankingResult()
        assert rr.ranked_problems == []
        assert rr.query_summary == ""
        assert rr.total_candidates == 0


# =============================================================================
# ExperimentalStep
# =============================================================================


class TestExperimentalStep:
    """Tests for ExperimentalStep model."""

    def test_valid_step(self):
        """Create valid step."""
        step = ExperimentalStep(
            step_number=1,
            description="Implement the sampling algorithm",
            expected_output="Python module",
        )
        assert step.step_number == 1

    def test_step_number_must_be_positive(self):
        """Step number must be >= 1."""
        with pytest.raises(ValidationError):
            ExperimentalStep(
                step_number=0,
                description="Invalid step number test",
                expected_output="Nothing",
            )

    def test_description_min_length(self):
        """Description must be at least 10 chars."""
        with pytest.raises(ValidationError):
            ExperimentalStep(
                step_number=1,
                description="short",
                expected_output="Nothing",
            )


# =============================================================================
# ContinuationProposal
# =============================================================================


class TestContinuationProposal:
    """Tests for ContinuationProposal model."""

    def test_create_valid(self):
        """Create with valid required fields."""
        cp = ContinuationProposal(
            problem_id="prob-1",
            title="Valid Title",
            methodology="A detailed methodology description here.",
            expected_outcome="Improved results across benchmarks",
        )
        assert cp.problem_id == "prob-1"
        assert cp.confidence == 0.5  # default

    def test_title_min_length(self):
        """Title must be >= 5 chars."""
        with pytest.raises(ValidationError):
            ContinuationProposal(
                problem_id="p1",
                title="Hi",
                methodology="A detailed methodology description here.",
                expected_outcome="Some outcome result",
            )

    def test_methodology_min_length(self):
        """Methodology must be >= 20 chars."""
        with pytest.raises(ValidationError):
            ContinuationProposal(
                problem_id="p1",
                title="Valid Title",
                methodology="Too short",
                expected_outcome="Some outcome result",
            )

    def test_steps_renumbered_sequentially(self):
        """Validator renumbers steps to be sequential."""
        cp = ContinuationProposal(
            problem_id="p1",
            title="Valid Title",
            methodology="A detailed methodology description here.",
            expected_outcome="Some outcome result",
            experimental_steps=[
                ExperimentalStep(
                    step_number=5,
                    description="First step but wrong number",
                    expected_output="Output 1",
                ),
                ExperimentalStep(
                    step_number=10,
                    description="Second step but wrong number",
                    expected_output="Output 2",
                ),
            ],
        )
        assert cp.experimental_steps[0].step_number == 1
        assert cp.experimental_steps[1].step_number == 2

    def test_confidence_range(self):
        """Confidence must be 0-1."""
        with pytest.raises(ValidationError):
            ContinuationProposal(
                problem_id="p1",
                title="Valid Title",
                methodology="A detailed methodology description here.",
                expected_outcome="Some outcome result",
                confidence=1.5,
            )


# =============================================================================
# EvaluationResult
# =============================================================================


class TestEvaluationResult:
    """Tests for EvaluationResult model."""

    def test_create_valid(self):
        """Create with valid data."""
        er = EvaluationResult(feasibility_score=0.7, verdict="promising")
        assert er.verdict == "promising"
        assert er.execution_success is False

    def test_verdict_normalized(self):
        """Verdict is lowered and stripped."""
        er = EvaluationResult(feasibility_score=0.5, verdict="  Promising  ")
        assert er.verdict == "promising"

    def test_verdict_with_spaces_normalized(self):
        """Verdict with spaces is converted to underscores."""
        er = EvaluationResult(feasibility_score=0.5, verdict="not viable")
        assert er.verdict == "not_viable"

    def test_invalid_verdict_becomes_inconclusive(self):
        """Unknown verdict defaults to inconclusive."""
        er = EvaluationResult(feasibility_score=0.5, verdict="excellent")
        assert er.verdict == "inconclusive"

    def test_empty_verdict_allowed(self):
        """Empty string verdict is allowed."""
        er = EvaluationResult(feasibility_score=0.5, verdict="")
        assert er.verdict == ""

    def test_feasibility_score_range(self):
        """Feasibility score must be 0-1."""
        with pytest.raises(ValidationError):
            EvaluationResult(feasibility_score=1.5, verdict="promising")


# =============================================================================
# MetricResult
# =============================================================================


class TestMetricResult:
    """Tests for MetricResult model."""

    def test_create_minimal(self):
        """Create with just name."""
        mr = MetricResult(name="accuracy")
        assert mr.value is None
        assert mr.improvement is None

    def test_create_full(self):
        """Create with all fields."""
        mr = MetricResult(
            name="accuracy",
            value=0.95,
            baseline_value=0.90,
            improvement=0.055,
            notes="Good improvement",
        )
        assert mr.value == 0.95


# =============================================================================
# SynthesisReport
# =============================================================================


class TestSynthesisReport:
    """Tests for SynthesisReport model."""

    def test_create_valid(self):
        """Create with valid summary."""
        sr = SynthesisReport(
            summary="This workflow investigated GNN scalability and found promising approaches.",
        )
        assert sr.new_problems == []
        assert sr.graph_updates == []

    def test_summary_min_length(self):
        """Summary must be >= 20 chars."""
        with pytest.raises(ValidationError):
            SynthesisReport(summary="Too short")


# =============================================================================
# GraphUpdate
# =============================================================================


class TestGraphUpdate:
    """Tests for GraphUpdate model."""

    def test_create_valid(self):
        """Create with valid data."""
        gu = GraphUpdate(action="create_problem", target_id="new-1", details="Created new problem")
        assert gu.action == "create_problem"


# =============================================================================
# HumanCheckpoint
# =============================================================================


class TestHumanCheckpoint:
    """Tests for HumanCheckpoint model."""

    def test_create_with_defaults(self):
        """Create with minimal fields."""
        hc = HumanCheckpoint(checkpoint_type=CheckpointType.SELECT_PROBLEM)
        assert hc.decision is None
        assert hc.data == {}
        assert isinstance(hc.timestamp, datetime)

    def test_create_with_decision(self):
        """Create with decision and feedback."""
        hc = HumanCheckpoint(
            checkpoint_type=CheckpointType.APPROVE_PROPOSAL,
            decision=CheckpointDecision.APPROVE,
            feedback="Looks good",
        )
        assert hc.decision == CheckpointDecision.APPROVE

    def test_serialization_roundtrip(self):
        """Model serializes and deserializes correctly."""
        hc = HumanCheckpoint(
            checkpoint_type=CheckpointType.REVIEW_EVALUATION,
            decision=CheckpointDecision.EDIT,
            edited_data={"verdict": "promising"},
        )
        data = hc.model_dump(mode="json")
        restored = HumanCheckpoint.model_validate(data)
        assert restored.checkpoint_type == CheckpointType.REVIEW_EVALUATION
        assert restored.edited_data == {"verdict": "promising"}


# =============================================================================
# WorkflowSummary
# =============================================================================


class TestWorkflowSummary:
    """Tests for WorkflowSummary model."""

    def test_create_valid(self):
        """Create with required fields."""
        ws = WorkflowSummary(
            run_id="run-123",
            status=WorkflowStatus.RUNNING,
        )
        assert ws.run_id == "run-123"
        assert ws.problem_count == 0
        assert ws.selected_problem is None
